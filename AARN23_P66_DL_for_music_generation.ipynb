{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm_JnY9cGPAW",
        "outputId": "75939b2b-4a19-4941-d1c3-6409ae0a9c94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.1.3-py3-none-any.whl (777 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.7/777.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.3.0-py3-none-any.whl (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pytorch-lightning) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pytorch-lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.10.0 pytorch-lightning-2.1.3 torchmetrics-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from torch.nn.utils.rnn import PackedSequence\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from typing import *\n"
      ],
      "metadata": {
        "id": "OwquRxR8HV5W"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "MAX_SEQ = 150\n"
      ],
      "metadata": {
        "id": "uKIn0SbqHyGn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def position_encoding_init(n_position, emb_dim):\n",
        "    \"\"\"Init the sinusoid position encoding table\"\"\"\n",
        "\n",
        "    # keep dim 0 for padding token position encoding zero vector\n",
        "    position_enc = torch.tensor(\n",
        "        [\n",
        "            [pos / np.power(10000, 2 * (j // 2) / emb_dim) for j in range(emb_dim)]\n",
        "            if pos != 0\n",
        "            else np.zeros(emb_dim)\n",
        "            for pos in range(n_position)\n",
        "        ],\n",
        "        dtype=torch.float32,\n",
        "    )\n",
        "\n",
        "    position_enc[1:, 0::2] = np.sin(\n",
        "        position_enc[1:, 0::2]\n",
        "    )  # apply sin on 0th,2nd,4th...emb_dim\n",
        "    position_enc[1:, 1::2] = np.cos(\n",
        "        position_enc[1:, 1::2]\n",
        "    )  # apply cos on 1st,3rd,5th...emb_dim\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        position_enc = position_enc.cuda()\n",
        "\n",
        "    return position_enc\n"
      ],
      "metadata": {
        "id": "oAZKok6GNu4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VariationalDropout(nn.Module):\n",
        "    \"\"\"\n",
        "    Applies the same dropout mask across the temporal dimension. See\n",
        "    https://arxiv.org/abs/1512.05287 for more details.\n",
        "\n",
        "    Note that this is not applied to the recurrent activations in the\n",
        "    LSTM like the above paper. Instead, it is applied to the inputs\n",
        "    and outputs of the recurrent layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dropout: float, batch_first: Optional[bool] = False):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if not self.training or self.dropout <= 0.0:\n",
        "            return x\n",
        "\n",
        "        is_packed = isinstance(x, PackedSequence)\n",
        "        if is_packed:\n",
        "            x, batch_sizes = x\n",
        "            max_batch_size = int(batch_sizes[0])\n",
        "        else:\n",
        "            batch_sizes = None\n",
        "            max_batch_size = x.size(0)\n",
        "\n",
        "        # Drop same mask across entire sequence\n",
        "        if self.batch_first:\n",
        "            m = x.new_empty(\n",
        "                max_batch_size, 1, x.size(2), requires_grad=False\n",
        "            ).bernoulli_(1 - self.dropout)\n",
        "        else:\n",
        "            m = x.new_empty(\n",
        "                1, max_batch_size, x.size(2), requires_grad=False\n",
        "            ).bernoulli_(1 - self.dropout)\n",
        "        x = x.masked_fill(m == 0, 0) / (1 - self.dropout)\n",
        "\n",
        "        if is_packed:\n",
        "            return PackedSequence(x, batch_sizes)\n",
        "        else:\n",
        "            return x\n"
      ],
      "metadata": {
        "id": "SIiFBGCiIBdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer_Model(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        nb_tags,\n",
        "        nb_layers=1,\n",
        "        pe_dim=0,\n",
        "        emb_dim=100,\n",
        "        batch_size=1,\n",
        "        seq_len=MAX_SEQ,\n",
        "        dropout=0.0,\n",
        "        encoder_only=True,\n",
        "    ):\n",
        "        super(Transformer_Model, self).__init__()\n",
        "\n",
        "        self.nb_layers = nb_layers\n",
        "        self.emb_dim = emb_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.seq_len = seq_len\n",
        "        self.pe_dim = pe_dim\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.nb_tags = nb_tags\n",
        "\n",
        "        self.encoder_only = encoder_only\n",
        "\n",
        "        # build actual NN\n",
        "        self.__build_model()\n",
        "\n",
        "    def __build_model(self):\n",
        "        self.embedding = nn.Embedding(self.nb_tags, self.emb_dim)\n",
        "\n",
        "        if not self.encoder_only:\n",
        "            self.embedding2 = nn.Embedding(self.nb_tags, self.emb_dim)\n",
        "\n",
        "        self.pos_emb = position_encoding_init(MAX_SEQ, self.pe_dim)\n",
        "        self.pos_emb.requires_grad = False\n",
        "\n",
        "        self.dropout_i = nn.Dropout(self.dropout)\n",
        "\n",
        "        input_size = self.pe_dim + self.emb_dim\n",
        "\n",
        "        self.transformerLayerI = nn.TransformerEncoderLayer(\n",
        "            d_model=input_size, nhead=8, dropout=self.dropout, dim_feedforward=1024\n",
        "        )\n",
        "\n",
        "        self.transformerI = nn.TransformerEncoder(\n",
        "            self.transformerLayerI,\n",
        "            num_layers=self.nb_layers,\n",
        "        )\n",
        "\n",
        "        self.dropout_m = nn.Dropout(self.dropout)\n",
        "\n",
        "        if not self.encoder_only:\n",
        "            # design decoder\n",
        "            self.transformerLayerO = nn.TransformerDecoderLayer(\n",
        "                d_model=input_size, nhead=8, dropout=self.dropout, dim_feedforward=1024\n",
        "            )\n",
        "\n",
        "            self.transformerO = nn.TransformerDecoder(\n",
        "                self.transformerLayerO,\n",
        "                num_layers=self.nb_layers,\n",
        "            )\n",
        "\n",
        "            self.dropout_o = nn.Dropout(self.dropout)\n",
        "\n",
        "        # output layer which projects back to tag space\n",
        "        self.hidden_to_tag = nn.Linear(self.emb_dim + self.pe_dim, self.nb_tags)\n",
        "\n",
        "    def __pos_encode(self, p):\n",
        "        return self.pos_emb[p]\n",
        "\n",
        "    def forward(self, X, p, X2=None, train_embedding=True):\n",
        "        self.embedding.weight.requires_grad = train_embedding\n",
        "        if not self.encoder_only:\n",
        "            self.embedding2.weight.requires_grad = train_embedding\n",
        "\n",
        "        I = X\n",
        "\n",
        "        self.mask = (torch.triu(torch.ones(self.seq_len, self.seq_len)) == 1).transpose(\n",
        "            0, 1\n",
        "        )\n",
        "        self.mask = (\n",
        "            self.mask.float()\n",
        "            .masked_fill(self.mask == 0, float(\"-inf\"))\n",
        "            .masked_fill(self.mask == 1, float(0.0))\n",
        "        )\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.mask = self.mask.cuda()\n",
        "\n",
        "        # ---------------------\n",
        "        # Combine inputs\n",
        "        X = self.embedding(I)\n",
        "        X = X.view(self.seq_len, self.batch_size, -1)\n",
        "\n",
        "        if self.pe_dim > 0:\n",
        "            P = self.__pos_encode(p)\n",
        "            P = P.view(self.seq_len, self.batch_size, -1)\n",
        "            X = torch.cat((X, P), 2)\n",
        "\n",
        "        X = self.dropout_i(X)\n",
        "\n",
        "        # Run through transformer encoder\n",
        "\n",
        "        M = self.transformerI(X, mask=self.mask)\n",
        "        M = self.dropout_m(M)\n",
        "\n",
        "        if not self.encoder_only:\n",
        "            # ---------------------\n",
        "            # Decoder stack\n",
        "            X = self.embedding2(X2)\n",
        "            X = X.view(self.seq_len, self.batch_size, -1)\n",
        "\n",
        "            if self.pe_dim > 0:\n",
        "                X = torch.cat((X, P), 2)\n",
        "\n",
        "            X = self.dropout_i(X)\n",
        "\n",
        "            X = self.transformerO(X, M, tgt_mask=self.mask, memory_mask=None)\n",
        "            X = self.dropout_o(X)\n",
        "\n",
        "            # run through linear layer\n",
        "            X = self.hidden_to_tag(X)\n",
        "        else:\n",
        "            X = self.hidden_to_tag(M)\n",
        "\n",
        "        Y_hat = X\n",
        "        return Y_hat\n",
        ""
      ],
      "metadata": {
        "id": "WBv1HzttHpbx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}